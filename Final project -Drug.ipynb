{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d499335-0b20-463b-b1bc-df2221588371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import string\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "text=\"This is first sentence. and this is another one. here 3rd one is  \"\n",
    "doc=nlp(text)\n",
    "doc\n",
    "for token in doc:\n",
    "    print(token)\n",
    "sent=nlp.create_pipe('sentencizer')\n",
    "import spacy\n",
    "from spacy.pipeline import Sentencizer\n",
    "# Load your spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Create the Sentencizer\n",
    "sentencizer = Sentencizer()\n",
    "# Add the Sentencizer to the pipeline before the parser\n",
    "nlp.add_pipe('sentencizer', before='parser')\n",
    "doc=nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords=list(STOP_WORDS)\n",
    "print(stopwords)\n",
    "len(stopwords)\n",
    "for token in doc:\n",
    "    if token.is_stop==False:\n",
    "        print(token)\n",
    "doc=nlp('run runs running runner')\n",
    "for lem in doc:\n",
    "    print(lem.text,lem.lemma_)\n",
    "doc=nlp('All is well at your end!')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "displacy.render(doc,style='dep')\n",
    "doc=nlp(\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest nat ional flash point over refusals to inoculate against dangerous diseases. At least 285 people have contracted measles in the city since September, mostly in Brooklyn's Williamsburg neighborhood. The order covers four Zi p codes there, Mayor Bill de Blasio (D) said Tuesday. The mandate orders all unvaccinated people in the area, including a concentration of Orthod ox Jews, to receive inoculations, including for children as young as 6 m onths old. Anyone who resists could be fined up to $1,000.\")\n",
    "doc\n",
    "\n",
    "displacy.render(doc,style='ent')\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "train_data = pd.read_csv(\"drugsComTrain_raw.tsv\", sep=\"\\t\")\n",
    "test_data = pd.read_csv(\"drugsComTest_raw.tsv\", sep=\"\\t\")\n",
    "data = pd.concat([train_data, test_data], ignore_index=True)\n",
    "data['review'][1]\n",
    "data = data[['review', 'rating']]\n",
    "data['Sentiment'] = data['rating'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "data = data.dropna(subset=['review', 'rating', 'Sentiment'])\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"Columns:\", data.columns)\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "print(\"Sentiment Value Counts:\")\n",
    "print(data['Sentiment'].value_counts())\n",
    "print(data.head())\n",
    "def fast_text_cleaning(sentence):\n",
    "    sentence = re.sub(r'\\W+', ' ', sentence.lower())  # Remove non-word characters\n",
    "    tokens = sentence.split()\n",
    "    return [token for token in tokens if token not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "\n",
    "print(\"Sample cleaned tokens:\", fast_text_cleaning(\"Wow! This is an amazing product. Totally recommend it!\"))\n",
    "X = data['review']\n",
    "Y = data['Sentiment']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "tfidf = TfidfVectorizer(tokenizer=fast_text_cleaning, token_pattern=None)\n",
    "#classifier = LinearSVC(max_iter=5000, dual=False)  # Faster optimization with more iterations\n",
    "classifier = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000, tol=1e-3)\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\n",
    "print(\"Training the model...\")\n",
    "clf.fit(X_train, Y_train)\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "# Precision\n",
    "precision = precision_score(Y_test, y_pred)\n",
    "# Recall\n",
    "recall = recall_score(Y_test, y_pred)\n",
    "# F1 Score\n",
    "f1 = f1_score(Y_test, y_pred)\n",
    "# Print results\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "# ACCURACY CHECK FOR NAIVE BAYES\n",
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=fast_text_cleaning, token_pattern=None)\n",
    "# Define NaÃ¯ve Bayes Classifier\n",
    "classifier = MultinomialNB()\n",
    "# Create a pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\n",
    "# Train the model\n",
    "clf.fit(X_train, Y_train)\n",
    "# Predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "# Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "# Compute Performance Metrics\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "precision = precision_score(Y_test, y_pred)\n",
    "recall = recall_score(Y_test, y_pred)\n",
    "f1 = f1_score(Y_test, y_pred)\n",
    "# Print Results\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall: {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n",
    "# Display Classification Report\n",
    "print(\"\\nðŸ”¹ Classification Report:\\n\", classification_report(Y_test, y_pred))\n",
    "# ðŸ”¹ Visualizing the Confusion Matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#!C:/Users/KIIT0001/Desktop/jupyter/Spacy_proj/spacy_venv/Scripts/python.exe -m pip install xgboost\n",
    "# Import necessary libraries\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=fast_text_cleaning, token_pattern=None)\n",
    "\n",
    "# Define XGBoost Classifier\n",
    "classifier = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "conf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# Compute Performance Metrics\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "precision = precision_score(Y_test, y_pred)\n",
    "recall = recall_score(Y_test, y_pred)\n",
    "f1 = f1_score(Y_test, y_pred)\n",
    "\n",
    "# Print Results\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall: {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Display Classification Report\n",
    "print(\"\\nðŸ”¹ Classification Report:\\n\", classification_report(Y_test, y_pred))\n",
    "\n",
    "# ðŸ”¹ Visualizing the Confusion Matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Model trained successfully.\")\n",
    "\n",
    "print(\"Making predictions on the test data...\")\n",
    "Y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "\n",
    "print(clf.predict(['Wow, this drug is amazing!']))  # Expected: Positive sentiment (1)\n",
    "\n",
    "\n",
    "print(clf.predict(['this drug is very terrible']))  # Expected: Negative sentiment (0)\n",
    "\n",
    "\n",
    "print(clf.predict(['sarat is a amazing guy']))  # Expected: Negative sentiment (0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_kernel",
   "language": "python",
   "name": "spacy_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
